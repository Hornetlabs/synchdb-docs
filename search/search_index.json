{"config":{"lang":["en","es","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>SynchDB is a PostgreSQL extension for synchronizing data from different database sources.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>SynchDB is a PostgreSQL extension designed to replicate data from one or more heterogeneous databases (such as MySQL, MS SQLServer, Oracle, etc.) directly to PostgreSQL in a fast and reliable way. PostgreSQL serves as the destination from multiple heterogeneous database sources. No middleware or third-party software is required to orchestrate the data synchronization between heterogeneous databases and PostgreSQL. SynchDB extension itself is capable of handling all the data synchronization needs.</p> <p>It provides two key work modes that can be invoked using the built-in SQL functions: - Sync mode (for initial data synchronization) - Follow mode (for replicate incremental changes after initial sync)</p> <p>Sync mode copies tables from heterogeneous database into PostgreSQL, including its schema, indexes, triggers, other table properties as well as current data it holds.</p> <p>Follow mode subscribes to tables in a heterogeneous database to obtain incremental changes and apply them to the same tables in PostgreSQL, similar to PostgreSQL logical replication</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Efficient data synchronization</li> <li>Support for multiple database sources</li> <li>Easy integration with existing PostgreSQL databases</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out our Installation Guide to get started with SynchDB.</p>"},{"location":"architecture/architecture/","title":"Architecture","text":""},{"location":"architecture/architecture/#archtecture-diagram","title":"Archtecture Diagram","text":"<p>SynchDB extension consists of six major components: * Debezium Runner Engine (Java) * SynchDB Launcher * SynchDB Worker * Format Converter * Replication Agent * Table Synch Agent (TBD)</p> <p>Refer to the architecture diagram for a visual representation of the components and their interactions. </p>"},{"location":"architecture/architecture/#debezium-runner-engine-java","title":"Debezium Runner Engine (Java)","text":"<ul> <li>A java application utilizing Debezium embedded library.</li> <li>Supports various connector implementations to replicate change data from various database types such as MySQL, Oracle, SQL Server, etc.</li> <li>Invoked by <code>SynchDB Worker</code> to initialize Debezium embedded library and receive change data.</li> <li>Send the change data to <code>SynchDB Worker</code> in generalized JSON format for further processing.</li> </ul>"},{"location":"architecture/architecture/#synchdb-launcher","title":"SynchDB Launcher","text":"<ul> <li>Responsible for creating and destroying SynchDB workers using PostgreSQL's background worker APIs.</li> <li>Configure each worker's connector type, destination database IPs, ports, etc.</li> </ul>"},{"location":"architecture/architecture/#synchdb-worker","title":"SynchDB Worker","text":"<ul> <li>Instantiates a <code>Debezium Runner Engine</code> to replicate changes from a specific connector type.</li> <li>Communicate with Debezium Runner via JNI to receive change data in JSON formats.</li> <li>Transfer the JSON change data to <code>Format Converter</code> module for further processing.</li> </ul>"},{"location":"architecture/architecture/#format-converter","title":"Format Converter","text":"<ul> <li>Parse the JSON change data using PostgreSQL Jsonb APIs</li> <li>Transform DDL change details to PostgreSQL compatible SQL queries following user-defined translation rules.</li> <li>Transform DML change details to PostgreSQL compatible data representation by processing them based on column data types. It Produces raw HeapTupleData which can be fed directly to Heap Access Method within PostgreSQL for faster executions.</li> </ul>"},{"location":"architecture/architecture/#replication-agent","title":"Replication Agent","text":"<ul> <li>Processes the outputs from <code>Format Converter</code>.</li> <li><code>Format Converter</code> will produce HeapTupleData format outputs, then <code>Replication Agent</code> will invoke PostgreSQL's heap access method routines to handle them.</li> <li>for DDL queries, <code>Replication Agent</code> will invoke PostgreSQL's SPI to handle them.</li> </ul>"},{"location":"architecture/architecture/#table-sync-agent","title":"Table Sync Agent","text":"<ul> <li>Design details and implementation are not available yet. TBD</li> <li>Intended to provide a more efficient alternative to perform initial table synchronization.</li> </ul>"},{"location":"architecture/architecture/#jave-native-interface-jni","title":"Jave Native Interface (JNI)","text":"<p>Java Native Interface (JNI) is a framework that allows Java applications to interact with native code written in languages like C or C++. It enables Java programs to call and be called by native applications and libraries, providing a bridge between Java\u2019s platform independence and the performance advantages of native code. JNI is commonly used to integrate platform-specific features, optimize performance-critical sections of an application, or access legacy libraries that are not available in Java. It requires careful management of resources, as it involves switching between the Java Virtual Machine (JVM) and native environments, which can introduce complexity.</p> <p>SynchDB requires JNI to exchange resources between Debezium runner engine and SynchDB PostgreSQL extension. JNI is available with Java Installation (ex. openjdk).</p>"},{"location":"architecture/batch_change_handling/","title":"Batch Change Handling","text":""},{"location":"architecture/batch_change_handling/#overview","title":"Overview","text":"<p>SynchDB periodically fetches a batch of change request from Debezium runner engine at a period of <code>synchdb.naptime</code> milliseconds (default 500). This batch of change request is then processed by SynchDB. If all the change requests within the batch have been processed successfully (parsed, transformed and applied to PostgreSQL), SynchDB will notify the Debezium runner engine that this batch has been completed. This signals Debezium runner to commit the offset up until the last successfully completed change record. With this mechanism in place, SynchDB is able to track each change record and instruct Debezium runner not to fetch an old change that has been processed before, or not to send a duplcate change record.</p>"},{"location":"architecture/batch_change_handling/#batch-handling-on-success","title":"Batch Handling on Success","text":"<p>If all change requests inside a batch have all been successfully processed, SynchDB simply sends a message to Debezium runner engine to mark the batch as processed and completed, which caused offsets to be committed and eventually flush to disk.</p> <p></p>"},{"location":"architecture/batch_change_handling/#batch-handling-on-partial-success","title":"Batch Handling on Partial Success","text":"<p>In the case when one of the change requests has failed to process due to internal PostgreSQL errors such as duplicate key violation, SynchDB will still notify Debezium runner engine about a partially completed batch. This notify message contains an indicator about the last successfully processed record. Debezium will then mark all the completed records as processed, while leaving unprocessed and failed records as they are. With this, when SynchDB and Debezium runner restart, they will resume from the last failed change record and will only proceed when the fault has been cleared.</p> <p></p>"},{"location":"architecture/metadata_files/","title":"Metadata Files","text":"<p>During Operation, Debezium Runner engine produces metadata files under $PGDATA/pg_synchdb. Currently 2 types of metadata files are generated and persisted: * offset file: contains the offset to resume the replication operation at start up * schema history file: contains the schema information to build all the tables for a replication. This is created during the initial data snapshot sync and can be updated during operation.</p> <p>These metadata filenames consist of: * (connector type)(connector name)_offsets.dat * (connector type)(connector name)_schemahistory.dat</p> <pre><code>ls $PGDATA/pg_synchdb\nmysql_mysqlconn_offsets.dat        sqlserver_sqlserverconn_offsets.dat\nmysql_mysqlconn_schemahistory.dat  sqlserver_sqlserverconn_schemahistory.dat\n</code></pre> <p>These binary files' contents can be viewed with hexdump command: <pre><code>hexdump -C $PGDATA/pg_synchdb/mysql_mysqlconn_offsets.dat\nhexdump -C $PGDATA/pg_synchdb/mysql_mysqlconn_schemahistory.dat\n</code></pre></p>"},{"location":"user-guide/","title":"The User guide Home","text":""},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>SynchDB supports the following GUC variables in postgresql.conf:</p> GUC type default value description synchdb.naptime integer 500 The delay in milliseconds between each data polling from Debezium runner engine synchdb.dml_use_spi boolean false Option to use SPI to handle DML operations synchdb.synchdb_auto_launcher boolean true Option to automatically launch active SynchDB connector workers. This option only works when SynchDB is included in <code>shared_preload_library</code> GUC option"},{"location":"user-guide/connector_auto_launcher/","title":"Connector Auto Launcher","text":""},{"location":"user-guide/connector_auto_launcher/#enable-synchdb-auto-launcher","title":"Enable SynchDB Auto Launcher","text":"<p>A connection worker becomes eligible for automatic Launch when <code>synchdb_start_engine_bgw()</code> is issued on a particular <code>connector name</code>. Likewise, it becomes ineligible when <code>synchdb_stop_engine_bgw()</code> is issued. </p> <p>Automatic connector launcher can be enabled by:</p> <ul> <li>add <code>synchdb</code> to <code>shared_preload_libraries</code> GUC option in postgresql.conf</li> <li>set new GUC option <code>synchdb.synchdb_auto_launcher</code> to true in postgresql.conf</li> <li>restart the PostgreSQL server for the changes to take effect</li> </ul> <p>For example: <pre><code>shared_preload_libraries = 'synchdb'\nsynchdb.synchdb_auto_launcher = true\n</code></pre></p> <p>At startup, SynchDB extension will be preloaded very early. With <code>synchdb.synchdb_auto_launcher</code> set to true, SynchDB will spawn a <code>synchdb_auto_launcher</code> background worker that will retrieve all the conninfos in <code>synchdb_conninfo</code> table that is marked as <code>active</code> ( has <code>isactive</code> flag set to <code>true</code>). Then, it will start them automatically as a separete background worker in the same way as when <code>synchdb_start_engine_bgw()</code> is called.. <code>synchdb_auto_launcher</code> will exit after.</p>"},{"location":"user-guide/connector_auto_launcher/#known-issue","title":"Known Issue","text":"<p><code>synchdb_auto_launcher</code> worker will login to the default <code>postgres</code> database and try to find active conenctors from <code>synchdb_conninfo</code> table. If SynchDB has been installed from non-default database, then <code>synchdb_auto_launcher</code> will fail to find the table, and thus not automatically starting the connector worker. In the future, we will make <code>synchdb_auto_launcher</code> check for all the databases and automatically start conenctor workers based on every database's <code>synchdb_conninfo</code> tables.</p>"},{"location":"user-guide/custom_datatype_mapping/","title":"Custom Data Type Mapping","text":""},{"location":"user-guide/custom_datatype_mapping/#custom-rule-file","title":"Custom Rule File","text":"<p>For each supported connector type, SynchDB has a default, built-in data type translation rule that maps heterogeneous data types to those compatible in PostgreSQL. This default rule can be partially overwritten with a custom JSON formatted rule file that is specified during <code>synchdb_add_conninfo()</code>. Synchdb seraches for specified rule file under $PGDATA directory where PostgreSQL server runs on. Here's an example of rule file:</p> <p>myrule.json <pre><code>{\n    \"translation_rules\":\n    [\n        {\n            \"translate_from\": \"LINESTRING\",\n            \"translate_from_autoinc\": false,\n            \"translate_to\": \"TEXT\",\n            \"translate_to_size\": 0\n        },\n        {\n            \"translate_from\": \"BIGINT UNSIGNED\",\n            \"translate_from_autoinc\": false,\n            \"translate_to\": \"NUMERIC\",\n            \"translate_to_size\": -1\n        }\n    ]\n}\n</code></pre></p> <p>The rule file must contain a JSON array with name <code>\"translation_rules\"</code> that contains multiple objects: * \"translate_from\": the data type name from heterogeneous database to translate from. * \"translate_from_autoinc\": indicate if the data type specified in <code>\"translate_from\"</code> is marked as auto increment. * \"translate_to\": the PostgreSQL data type to translate to. See below for list of supported PostgreSQL data types. * \"translate_to_size\": indicate if we should transform the size of the data type specifed in <code>\"translate_to\"</code>. For example:     * 0: Remove any length specifier because the PostgreSQL data type translated to does not need length specifier. For example: TEXT     * -1: Use whatever length specifier from the source heterogeneous database (if available) and map it directly to the translated datatype. For example: VARCHAR.     * others: put any other values to force the size of translated data type. Use it with caution.</p>"},{"location":"user-guide/custom_datatype_mapping/#supported-postgresql-data-type","title":"Supported PostgreSQL Data Type","text":"<p>PostgreSQL supports a range of data types from native to complex data types. However, SyncDB only supports a subset of PostgreSQL data types as of now. More data type support is in the plan. This means that when defining a custom data type mapping rule file, we need to ensure the <code>translate_to</code> data type falls within the list of supported PostgreSQL data type below:</p> <ul> <li>BOOLEAN (BOOLOID)</li> <li>BIGINT (INT8OID)</li> <li>SMALLINT (INT2OID)</li> <li>INT (INT4OID)</li> <li>INTEGER (INT4OID)</li> <li>DOUBLE PRECISION (FLOAT8OID)</li> <li>REAL (FLOAT4OID)</li> <li>MONEY (MONEYOID)</li> <li>NUMERIC (NUMERICOID)</li> <li>CHAR (BPCHAROID)</li> <li>CHARACTER (BPCHAROID)</li> <li>TEXT (TEXTOID)</li> <li>VARCHAR (VARCHAROID)</li> <li>CHARACTER VARYING (VARCHAROID)</li> <li>TIMESTAMPTZ (TIMESTAMPTZOID)</li> <li>JSONB (JSONBOID)</li> <li>UUID (UUIDOID)</li> <li>VARBIT (VARBITOID)</li> <li>BIT VARYING (VARBITOID)</li> <li>BIT (BITOID)</li> <li>DATE (DATEOID)</li> <li>TIMESTAMP (TIMESTAMPOID)</li> <li>TIME (TIMEOID)</li> <li>BYTEA (BYTEAOID)</li> </ul>"},{"location":"user-guide/custom_datatype_mapping/#next-steps","title":"Next Steps","text":"<p>The data type translation rule currently applies to all. In the future, we will have more granular control of the data type mapping by allowing a data type translation to happen only within specific tables or columns. Or automatically apply a function or expression to the value after the translation. (To be Determined)</p>"},{"location":"user-guide/default_datatype_mapping/","title":"Default Data Type Mapping","text":"<p>The default data type mapping is a hash table built internally inside SynchDB with: * key: {data type, auto_increment}  * value: {data type, size}</p> <p>where: * data type: the string representation of a data type such as INT, TEXT, NUMERIC * auto_increment: a flag indicating if the data type has auto_increment attribute. * size: the translated size value. -1: no change, 0: no size specified</p>"},{"location":"user-guide/default_datatype_mapping/#mysql-default-data-type-mapping","title":"MySQL Default Data Type Mapping","text":"<pre><code>DatatypeHashEntry mysql_defaultTypeMappings[] =\n{\n    {{\"INT\", true}, \"SERIAL\", 0},\n    {{\"BIGINT\", true}, \"BIGSERIAL\", 0},\n    {{\"SMALLINT\", true}, \"SMALLSERIAL\", 0},\n    {{\"MEDIUMINT\", true}, \"SERIAL\", 0},\n    {{\"ENUM\", false}, \"TEXT\", 0},\n    {{\"SET\", false}, \"TEXT\", 0},\n    {{\"BIGINT\", false}, \"BIGINT\", 0},\n    {{\"BIGINT UNSIGNED\", false}, \"NUMERIC\", -1},\n    {{\"NUMERIC UNSIGNED\", false}, \"NUMERIC\", -1},\n    {{\"DEC\", false}, \"DECIMAL\", -1},\n    {{\"DEC UNSIGNED\", false}, \"DECIMAL\", -1},\n    {{\"DECIMAL UNSIGNED\", false}, \"DECIMAL\", -1},\n    {{\"FIXED\", false}, \"DECIMAL\", -1},\n    {{\"FIXED UNSIGNED\", false}, \"DECIMAL\", -1},\n    {{\"BIT(1)\", false}, \"BOOLEAN\", 0},\n    {{\"BIT\", false}, \"BIT\", -1},\n    {{\"BOOL\", false}, \"BOOLEAN\", -1},\n    {{\"DOUBLE\", false}, \"DOUBLE PRECISION\", 0},\n    {{\"DOUBLE PRECISION\", false}, \"DOUBLE PRECISION\", 0},\n    {{\"DOUBLE PRECISION UNSIGNED\", false}, \"DOUBLE PRECISION\", 0},\n    {{\"DOUBLE UNSIGNED\", false}, \"DOUBLE PRECISION\", 0},\n    {{\"REAL\", false}, \"REAL\", 0},\n    {{\"REAL UNSIGNED\", false}, \"REAL\", 0},\n    {{\"FLOAT\", false}, \"REAL\", 0},\n    {{\"FLOAT UNSIGNED\", false}, \"REAL\", 0},\n    {{\"INT\", false}, \"INT\", 0},\n    {{\"INT UNSIGNED\", false}, \"BIGINT\", 0},\n    {{\"INTEGER\", false}, \"INT\", 0},\n    {{\"INTEGER UNSIGNED\", false}, \"BIGINT\", 0},\n    {{\"MEDIUMINT\", false}, \"INT\", 0},\n    {{\"MEDIUMINT UNSIGNED\", false}, \"INT\", 0},\n    {{\"YEAR\", false}, \"INT\", 0},\n    {{\"SMALLINT\", false}, \"SMALLINT\", 0},\n    {{\"SMALLINT UNSIGNED\", false}, \"INT\", 0},\n    {{\"TINYINT\", false}, \"SMALLINT\", 0},\n    {{\"TINYINT UNSIGNED\", false}, \"SMALLINT\", 0},\n    {{\"DATETIME\", false}, \"TIMESTAMP\", -1},\n    {{\"TIMESTAMP\", false}, \"TIMESTAMPTZ\", -1},\n    {{\"BINARY\", false}, \"BYTEA\", 0},\n    {{\"VARBINARY\", false}, \"BYTEA\", 0},\n    {{\"BLOB\", false}, \"BYTEA\", 0},\n    {{\"MEDIUMBLOB\", false}, \"BYTEA\", 0},\n    {{\"LONGBLOB\", false}, \"BYTEA\", 0},\n    {{\"TINYBLOB\", false}, \"BYTEA\", 0},\n    {{\"LONG VARCHAR\", false}, \"TEXT\", -1},\n    {{\"LONGTEXT\", false}, \"TEXT\", -1},\n    {{\"MEDIUMTEXT\", false}, \"TEXT\", -1},\n    {{\"TINYTEXT\", false}, \"TEXT\", -1},\n    {{\"JSON\", false}, \"JSONB\", -1},\n\n    /* spatial types - map to TEXT by default */\n    {{\"GEOMETRY\", false}, \"TEXT\", -1},\n    {{\"GEOMETRYCOLLECTION\", false}, \"TEXT\", -1},\n    {{\"GEOMCOLLECTION\", false}, \"TEXT\", -1},\n    {{\"LINESTRING\", false}, \"TEXT\", -1},\n    {{\"MULTILINESTRING\", false}, \"TEXT\", -1},\n    {{\"MULTIPOINT\", false}, \"TEXT\", -1},\n    {{\"MULTIPOLYGON\", false}, \"TEXT\", -1},\n    {{\"POINT\", false}, \"TEXT\", -1},\n    {{\"POLYGON\", false}, \"TEXT\", -1}\n};\n</code></pre>"},{"location":"user-guide/default_datatype_mapping/#sql-server-default-data-type-mapping","title":"SQL Server Default Data Type Mapping","text":"<p><code>`` DatatypeHashEntry sqlserver_defaultTypeMappings[] = {     {{\"int identity\", true}, \"SERIAL\", 0},     {{\"bigint identity\", true}, \"BIGSERIAL\", 0},     {{\"smallint identity\", true}, \"SMALLSERIAL\", 0},     {{\"enum\", false}, \"TEXT\", 0},     {{\"int\", false}, \"INT\", 0},     {{\"bigint\", false}, \"BIGINT\", 0},     {{\"smallint\", false}, \"SMALLINT\", 0},     {{\"tinyint\", false}, \"SMALLINT\", 0},     {{\"numeric\", false}, \"NUMERIC\", 0},     {{\"decimal\", false}, \"NUMERIC\", 0},     {{\"bit(1)\", false}, \"BOOL\", 0},     {{\"bit\", false}, \"BIT\", 0},     {{\"money\", false}, \"MONEY\", 0},     {{\"smallmoney\", false}, \"MONEY\", 0},     {{\"real\", false}, \"REAL\", 0},     {{\"float\", false}, \"REAL\", 0},     {{\"date\", false}, \"DATE\", 0},     {{\"time\", false}, \"TIME\", 0},     {{\"datetime\", false}, \"TIMESTAMP\", 0},     {{\"datetime2\", false}, \"TIMESTAMP\", 0},     {{\"datetimeoffset\", false}, \"TIMESTAMPTZ\", 0},     {{\"smalldatetime\", false}, \"TIMESTAMP\", 0},     {{\"char\", false}, \"CHAR\", 0},     {{\"varchar\", false}, \"VARCHAR\", -1},     {{\"text\", false}, \"TEXT\", 0},     {{\"nchar\", false}, \"CHAR\", 0},     {{\"nvarchar\", false}, \"VARCHAR\", -1},     {{\"ntext\", false}, \"TEXT\", 0},     {{\"binary\", false}, \"BYTEA\", 0},     {{\"varbinary\", false}, \"BYTEA\", 0},     {{\"image\", false}, \"BYTEA\", 0},     {{\"uniqueidentifier\", false}, \"UUID\", 0},     {{\"xml\", false}, \"TEXT\", 0},     /* spatial types - map to TEXT by default */     {{\"geometry\", false}, \"TEXT\", 0},     {{\"geography\", false}, \"TEXT\", 0}, };</code></p>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<p>The following software is required to build and run SynchDB. The versions listed are the versions tested during development. Older versions may still work. * Java Development Kit 22. Download here * Apache Maven 3.9.8. Download here * PostgreSQL 16.3 Source. Git clone here. Refer to this wiki for PostgreSQL build requirements * Docker compose 2.28.1 (for testing). Refer to here * Unix based operating system like Ubuntu 22.04 or MacOS</p>"},{"location":"user-guide/installation/#prepare-source","title":"Prepare Source","text":"<p>Clone the PostgreSQL source and switch to 16.3 release tag <pre><code>git clone https://github.com/postgres/postgres.git\ncd postgres\ngit checkout REL_16_3\n</code></pre></p> <p>Clone the SynchDB source from within the extension folder Note: Branch (synchdb-devel)[https://github.com/Hornetlabs/synchdb/tree/synchdb-devel] is used for development so far. <pre><code>cd contrib/\ngit clone https://github.com/Hornetlabs/synchdb.git\n</code></pre></p>"},{"location":"user-guide/installation/#prepare-tools","title":"Prepare Tools","text":""},{"location":"user-guide/installation/#install-maven","title":"Install Maven","text":"<p>If you are working on Ubuntu 22.04.4 LTS, install the Maven as below: <pre><code>sudo apt install maven\n</code></pre></p> <p>if you are using MacOS, you can use the brew command to install maven (refer (here)[https://brew.sh/] for how to install Homebrew) without any other settings: <pre><code>brew install maven\n</code></pre></p>"},{"location":"user-guide/installation/#install-java-sdk-openjdk","title":"Install Java SDK (OpenJDK)","text":"<p>If you are working on Ubuntu 22.04.4 LTS, install the OpenJDK  as below: <pre><code>sudo apt install openjdk-21-jdk\n</code></pre></p> <p>If you are working on MacOS, please install the JDK with brew command: <pre><code>brew install openjdk@22\n</code></pre></p>"},{"location":"user-guide/installation/#build-and-install-postgresql","title":"Build and Install PostgreSQL","text":"<p>Follow the official PostgreSQL documentation here to build and install PostgreSQL from source. Generally, the procedure consists of:</p> <pre><code>cd /home/$USER/postgres\n./configure\nmake\nsudo make install\n</code></pre> <p>You should build and install the default extensions as well: <pre><code>cd /home/$USER/postgres/contrib\nmake\nsudo make install\n</code></pre></p>"},{"location":"user-guide/installation/#build-and-install-debezium-runner-engine","title":"Build and Install Debezium Runner Engine","text":"<p>With Java and Maven setup, we are ready to build Debezium Runner Engine. This installs the Debezium Runner Engine jar file to your PostgreSQL's lib folder.</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake build_dbz\nsudo make install_dbz\n</code></pre>"},{"location":"user-guide/installation/#build-and-install-synchdb-postgresql-extension","title":"Build and Install SynchDB PostgreSQL Extension","text":"<p>With the Java <code>lib</code> and <code>include</code> installed in your system, SynchDB can be built by:</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake\nsudo make install\n</code></pre>"},{"location":"user-guide/installation/#configure-your-linker-ubuntu","title":"Configure Your Linker (Ubuntu)","text":"<p>Lastly, we also need to tell your system's linker where the newly added Java library is located in your system. The following procedure is based on Ubuntu 22.04.</p> <p><pre><code># Dynamically set JDK paths\nJAVA_PATH=$(which java)\nJDK_HOME_PATH=$(readlink -f ${JAVA_PATH} | sed 's:/bin/java::')\nJDK_LIB_PATH=${JDK_HOME_PATH}/lib\n\necho $JDK_LIB_PATH\necho $JDK_LIB_PATH/server\n\nsudo echo \"$JDK_LIB_PATH\" \uff5c sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\nsudo echo \"$JDK_LIB_PATH/server\" | sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\n</code></pre> Note, for mac with M1/M2 chips, you need to the two lines into /etc/ld.so.conf.d/aarch64-linux-gnu.conf</p> <p>Run ldconfig to reload: <pre><code>sudo ldconfig\n</code></pre></p>"},{"location":"user-guide/installation/#check-installation","title":"Check Installation","text":"<p>Ensure synchdo.so extension can link to libjvm Java library on your system: <pre><code>ldd synchdb.so\n        linux-vdso.so.1 (0x00007ffeae35a000)\n        libjvm.so =&gt; /usr/lib/jdk-22.0.1/lib/server/libjvm.so (0x00007fc1276c1000)\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc127498000)\n        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc127493000)\n        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc12748e000)\n        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc127489000)\n        libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc1273a0000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fc128b81000)\n</code></pre></p>"},{"location":"user-guide/prepare_tests_env/","title":"Prepare Sample Heterogeneous Databases","text":"<p>The procedures mentioned here are meant to quickly start up heterogeneous for quick verification and feature demostration. The necessary files and scripts for starting sample heterogeneous databases can be found at SynchDB repository here</p>"},{"location":"user-guide/prepare_tests_env/#prepare-a-sample-mysql-database","title":"Prepare a Sample MySQL Database","text":"<p>We can start a sample MySQL database for testing using docker compose. The user credentials are described in the <code>synchdb-mysql-test.yaml</code> file <pre><code>docker compose -f synchdb-mysql-test.yaml up -d\n</code></pre></p> <p>Login to MySQL as <code>root</code> and grant permissions to user <code>mysqluser</code> to perform real-time CDC <pre><code>mysql -h 127.0.0.1 -u root -p\n\nGRANT replication client on *.* to mysqluser;\nGRANT replication slave  on *.* to mysqluser;\nGRANT RELOAD ON *.* TO 'mysqluser'@'%';\nFLUSH PRIVILEGES;\n</code></pre></p> <p>Exit mysql client tool: <pre><code>\\q\n</code></pre></p>"},{"location":"user-guide/prepare_tests_env/#prepare-a-sample-sql-server-database","title":"Prepare a Sample SQL Server Database","text":"<p>We can start a sample SQL Server database for testing using docker compose. The user credentials are described in the <code>synchdb-sqlserver-test.yaml</code> file <pre><code>docker compose -f synchdb-sqlserver-test.yaml up -d\n</code></pre> use synchdb-sqlserver-withssl-test.yaml file for the SQL Server with SSL certificate enabled.</p> <p>You may not have SQL Server client tool installed, you could login to SQL Server container to access its client tool.</p> <p>Find out the container ID for SQL server: <pre><code>id=$(docker ps | grep sqlserver | awk '{print $1}')\n</code></pre></p> <p>Copy the database schema into SQL Server container: <pre><code>docker cp inventory.sql $id:/\n</code></pre></p> <p>Log in to SQL Server container: <pre><code>docker exec -it $id bash\n</code></pre></p> <p>Build the database according to the schema: <pre><code>/opt/mssql-tools/bin/sqlcmd -U sa -P $SA_PASSWORD -i /inventory.sql\n</code></pre></p> <p>Run some simple queries (add -N -C if you are using SSL enabled SQL Server): <pre><code>/opt/mssql-tools/bin/sqlcmd -U sa -P $SA_PASSWORD -d testDB -Q \"insert into orders(order_date, purchaser, quantity, product_id) values( '2024-01-01', 1003, 2, 107)\"\n\n/opt/mssql-tools/bin/sqlcmd -U sa -P $SA_PASSWORD -d testDB -Q \"select * from orders\"\n</code></pre></p>"},{"location":"user-guide/quick_start/","title":"Quick Start Guide","text":"<p>It is very simple to start using SynchDB to perform data replication from heterogeneous databases to PostgreSQL given that you have the correct connection information to your heterogeneous databases.</p>"},{"location":"user-guide/quick_start/#install-synchdb-extension","title":"Install SynchDB Extension","text":"<p>SynchDB extension requires pgcrypto to encrypt certain sensitive credential data. Please make sure it is installed prior to installing SynchDB. Alternatively, you can include <code>CASCADE</code> clause in <code>CREATE EXTENSION</code> to automatically install dependencies:</p> <pre><code>CREATE EXTENSION synchdb CASCADE;\n</code></pre>"},{"location":"user-guide/quick_start/#create-a-connection-info","title":"Create a Connection Info","text":"<p>This can be done with utility SQL function <code>synchdb_add_conninfo()</code>.</p> <p>synchdb_add_conninfo takes these arguments:</p> <ul> <li><code>name</code> - a unique identifier that represents this connection info</li> <li><code>hostname</code> - the IP address of the heterogeneous database.</li> <li><code>port</code> - the port number to connect to the heterogeneous database.</li> <li><code>username</code> - user name to use.</li> <li><code>password</code> - password to authenticate the username.</li> <li><code>source database</code> - this is the name of source database in heterogeneous database that we want to replicate changes from.</li> <li><code>destination</code> database - this is the name of destination database in PostgreSQL to apply changes to. It must be a valid database that exists in PostgreSQL.</li> <li><code>table</code> (optional) - expressed in the form of <code>[database].[table]</code> or <code>[database].[schema].[table]</code> that must exists in heterogeneous database so the engine will only replicate the specified tables. If left empty, all tables are replicated. Please note that tables not selected here will have their table schema replicated in PostgreSQL as well, however, only the data of the selected table will have their data replicated.</li> <li><code>connector</code> - the connector type to use (MySQL, Oracle, SQLServer... etc).</li> <li><code>rule file</code> - a JSON-formatted rule file placed under $PGDATA that this connector shall apply to its default data type translation rules.</li> </ul> <p>Examples:</p> <ol> <li> <p>Create a MySQL connector called 'mysqlconn' to replicate from source database 'inventory' in MySQL to destination database 'postgres' in PostgreSQL, using rule file <code>myrule.json</code>: <pre><code>SELECT synchdb_add_conninfo('mysqlconn', '127.0.0.1', 3306, 'mysqluser', 'mysqlpwd', 'inventory', 'postgres', '', 'mysql', 'myrule.json');\n</code></pre></p> </li> <li> <p>Create a MySQL connector called 'mysqlconn2' to replicate from source database 'inventory' to destination database 'mysqldb2' in PostgreSQL using default transaltion rule: <pre><code>SELECT synchdb_add_conninfo('mysqlconn2', '127.0.0.1', 3306, 'mysqluser', 'mysqlpwd', 'inventory', 'mysqldb2', '', 'mysql', '');\n</code></pre></p> </li> <li> <p>Create a SQLServer connector called 'sqlserverconn' to replicate from source database 'testDB' to destination database 'sqlserverdb' in PostgreSQL using default translation rule: <pre><code>SELECT synchdb_add_conninfo('sqlserverconn', '127.0.0.1', 1433, 'sa', 'Password!', 'testDB', 'sqlserverdb', '', 'sqlserver', '');\n</code></pre></p> </li> <li> <p>Create a MySQL connector called 'mysqlconn3' to replicate from source database 'inventory's <code>orders</code> and <code>customers</code> tabls to destination database 'mysqldb3' in PostgreSQL using rule file <code>myrule2.json</code>: <pre><code>SELECT synchdb_add_conninfo('mysqlconn3', '127.0.0.1', 3306, 'mysqluser', 'mysqlpwd', 'inventory', 'mysqldb3', 'inventory.orders,inventory.customers', 'mysql', 'myrule2.json');\n</code></pre></p> </li> </ol>"},{"location":"user-guide/quick_start/#things-to-note","title":"Things to Note","text":"<ul> <li>It is possible to create multiple connectors connecting to the same connector type (ex, MySQL, SQLServer..etc). SynchDB will spawn separate connections to fetch change data.</li> <li>Avoid creating 2 connectors connecting to the same heterogeneous database, same source database, tables and the same destination database. This could cause a conflict when both connectors start. In the future, we will add a column and table name mappings feature to allow a source table to be mapped to a different name on the destination to prevent such name conflict. In the meantime, please use different destination database instead.</li> <li>User-defined X509 certificate and private key for TLS connection to remote database will be supported in near future. In the meantime, please ensure TLS settings are set to optional.</li> </ul>"},{"location":"user-guide/quick_start/#check-created-connection-info","title":"Check Created Connection Info","text":"<p>All connection information are created in the table <code>synchdb_conninfo</code>. We are free to view its content and make modification as required. Please note that the password of a user credential is encrypted by pgcrypto using a key only known to synchdb. So please do not modify the password field or it may be decrypted incorrectly if tempered. See below for an example output:</p> <pre><code>postgres=# \\x\nExpanded display is on.\n\npostgres=# select * from synchdb_conninfo;\n-[ RECORD 1 ]-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nname | mysqlconn\ndata | {\"pwd\": \"\\\\xc30d040703024828cc4d982e47b07bd23901d03e40da5995d2a631fb89d49f748b87247aee94070f71ecacc4990c3e71cad9f68d57c440de42e35bcc78fd145feab03452e454284289db\", \"port\": 3306, \"user\": \"mysqluser\", \"dstdb\": \"postgres\", \"srcdb\": \"inventory\", \"table\": \"null\", \"hostname\": \"192.168.1.86\", \"connector\": \"mysql\"i, \"myrule.json\"}\n-[ RECORD 2 ]-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nname | sqlserverconn\ndata | {\"pwd\": \"\\\\xc30d0407030231678e1bb0f8d3156ad23a010ca3a4b0ad35ed148f8181224885464cdcfcec42de9834878e2311b343cd184fde65e0051f75d6a12d5c91d0a0403549fe00e4219215eafe1b\", \"port\": 1433, \"user\": \"sa\", \"dstdb\": \"sqlserverdb\", \"srcdb\": \"testDB\", \"table\": \"null\", \"hostname\": \"192.168.1.86\", \"connector\": \"sqlserver\", \"null\"}\n</code></pre>"},{"location":"user-guide/quick_start/#start-a-connector","title":"Start a Connector","text":"<p>Use <code>synchdb_start_engine_bgw()</code> function to start a connector worker. It takes one argument which is the connection  name created above. This command will spawn a new background worker to connect to the heterogeneous database with the specified configurations.</p> <p>For example, the following will spawn 2 background worker in PostgreSQL, one replicating from a MySQL database, the other from SQL Server</p> <pre><code>select synchdb_start_engine_bgw('mysqlconn');\nselect synchdb_start_engine_bgw('sqlserverconn');\n</code></pre>"},{"location":"user-guide/quick_start/#check-connector-running-state","title":"Check Connector Running State","text":"<p>Use <code>synchdb_state_view()</code> view to examine all the running connectors and their states. Currently, synchdb can support up to 30 running workers.</p> <p>See below for an example output: <pre><code>postgres=# select * from synchdb_state_view;\n id | connector | conninfo_name  |  pid   |  state  |   err    |                                          last_dbz_offset\n----+-----------+----------------+--------+---------+----------+---------------------------------------------------------------------------------------------------\n  0 | mysql     | mysqlconn      | 461696 | syncing | no error | {\"ts_sec\":1725644339,\"file\":\"mysql-bin.000004\",\"pos\":138466,\"row\":1,\"server_id\":223344,\"event\":2}\n  1 | sqlserver | sqlserverconn  | 461739 | syncing | no error | {\"event_serial_no\":1,\"commit_lsn\":\"00000100:00000c00:0003\",\"change_lsn\":\"00000100:00000c00:0002\"}\n  3 | null      |                |     -1 | stopped | no error | no offset\n  4 | null      |                |     -1 | stopped | no error | no offset\n  4 | null      |                |     -1 | stopped | no error | no offset\n  5 | null      |                |     -1 | stopped | no error | no offset\n  6 | null      |                |     -1 | stopped | no error | no offset\n  7 | null      |                |     -1 | stopped | no error | no offset\n  8 | null      |                |     -1 | stopped | no error | no offset\n  9 | null      |                |     -1 | stopped | no error | no offset\n 10 | null      |                |     -1 | stopped | no error | no offset\n 11 | null      |                |     -1 | stopped | no error | no offset\n 12 | null      |                |     -1 | stopped | no error | no offset\n 13 | null      |                |     -1 | stopped | no error | no offset\n 14 | null      |                |     -1 | stopped | no error | no offset\n 15 | null      |                |     -1 | stopped | no error | no offset\n 16 | null      |                |     -1 | stopped | no error | no offset\n 17 | null      |                |     -1 | stopped | no error | no offset\n 18 | null      |                |     -1 | stopped | no error | no offset\n 19 | null      |                |     -1 | stopped | no error | no offset\n 20 | null      |                |     -1 | stopped | no error | no offset\n 21 | null      |                |     -1 | stopped | no error | no offset\n 22 | null      |                |     -1 | stopped | no error | no offset\n 23 | null      |                |     -1 | stopped | no error | no offset\n 24 | null      |                |     -1 | stopped | no error | no offset\n 25 | null      |                |     -1 | stopped | no error | no offset\n 26 | null      |                |     -1 | stopped | no error | no offset\n 27 | null      |                |     -1 | stopped | no error | no offset\n 28 | null      |                |     -1 | stopped | no error | no offset\n 29 | null      |                |     -1 | stopped | no error | no offset\n</code></pre></p> <p>Column Details: * id: unique identifier of a connector slot * connector: the type of connector (mysql, oracle, sqlserver...etc) * conninfo_name: the associated connection info name created by <code>synchdb_add_conninfo()</code> * pid: the PID of the connector worker process * state: the state of the connector. Possible states are:     * stopped     * initializing     * paused     * syncing     * parsing     * converting     * executing     * updating offset     * unknown * err: the last error message encountered by the worker which would have caused it to exit. This error could originated from PostgreSQL while processing a change, or originated from Debezium running engine while accessing data from heterogeneous database. * last_dbz_offset: the last Debezium offset captured by synchdb. Note that this may not reflect the current and real-time offset value of the connector engine. Rather, this is shown as a checkpoint that we could restart from this offeet point if needed.</p>"},{"location":"user-guide/quick_start/#stop-a-connector","title":"Stop a Connector","text":"<p>Use <code>synchdb_stop_engine_bgw()</code> SQL function to stop a running or paused connector worker. This function takes <code>conninfo_name</code> as its only parameter, which can be found from the output of <code>synchdb_get_state()</code> view.</p> <p>For example: <pre><code>select synchdb_stop_engine_bgw('mysqlconn');\n</code></pre></p> <p><code>synchdb_stop_engine_bgw()</code> function also marks a connection info as <code>inactive</code>, which prevents the this worker from automatic-relaunch at server restarts. See below for more details.</p>"},{"location":"user-guide/set_offset/","title":"Set Custom Start Offset Values","text":"<p>A start offset value represents a point to start replication from in the similar way as PostgreSQL's resume LSN. When Debezium runner engine starts, it will start the replication from this offset value. Setting this offset value to a earlier value will cause Debezium runner engine to start replication from earlier records, possibly replicating duplicate data records. We should be extra cautious when setting start offset values on Debezium.</p>"},{"location":"user-guide/set_offset/#record-settable-offset-values","title":"Record Settable Offset Values","text":"<p>During operation, new offsets will be generated nd flushed to disk by Debezium runner engine. The last flushed offset can be retrieved from <code>synchdb_state_view()</code> utility command:</p> <pre><code>postgres=# select * from synchdb_state_view;\n id | connector | conninfo_name  |  pid   |  state  |   err    |                                          last_dbz_offset\n----+-----------+----------------+--------+---------+----------+---------------------------------------------------------------------------------------------------\n  0 | mysql     | mysqlconn      | 461696 | syncing | no error | {\"ts_sec\":1725644339,\"file\":\"mysql-bin.000004\",\"pos\":138466,\"row\":1,\"server_id\":223344,\"event\":2}\n  1 | sqlserver | sqlserverconn  | 461739 | syncing | no error | {\"event_serial_no\":1,\"commit_lsn\":\"00000100:00000c00:0003\",\"change_lsn\":\"00000100:00000c00:0002\"}\n  3 | null      |                |     -1 | stopped | no error | no offset\n  4 | null      |                |     -1 | stopped | no error | no offset\n  4 | null      |                |     -1 | stopped | no error | no offset\n  5 | null      |                |     -1 | stopped | no error | no offset\n  6 | null      |                |     -1 | stopped | no error | no offset\n  7 | null      |                |     -1 | stopped | no error | no offset\n  8 | null      |                |     -1 | stopped | no error | no offset\n  9 | null      |                |     -1 | stopped | no error | no offset\n 10 | null      |                |     -1 | stopped | no error | no offset\n 11 | null      |                |     -1 | stopped | no error | no offset\n 12 | null      |                |     -1 | stopped | no error | no offset\n 13 | null      |                |     -1 | stopped | no error | no offset\n 14 | null      |                |     -1 | stopped | no error | no offset\n 15 | null      |                |     -1 | stopped | no error | no offset\n 16 | null      |                |     -1 | stopped | no error | no offset\n 17 | null      |                |     -1 | stopped | no error | no offset\n 18 | null      |                |     -1 | stopped | no error | no offset\n 19 | null      |                |     -1 | stopped | no error | no offset\n 20 | null      |                |     -1 | stopped | no error | no offset\n 21 | null      |                |     -1 | stopped | no error | no offset\n 22 | null      |                |     -1 | stopped | no error | no offset\n 23 | null      |                |     -1 | stopped | no error | no offset\n 24 | null      |                |     -1 | stopped | no error | no offset\n 25 | null      |                |     -1 | stopped | no error | no offset\n 26 | null      |                |     -1 | stopped | no error | no offset\n 27 | null      |                |     -1 | stopped | no error | no offset\n 28 | null      |                |     -1 | stopped | no error | no offset\n 29 | null      |                |     -1 | stopped | no error | no offset\n</code></pre> <p>Depending on the connector type, this offset value differs. From the example above, the <code>mysql</code> connector's last flushed offset is <code>{\"ts_sec\":1725644339,\"file\":\"mysql-bin.000004\",\"pos\":138466,\"row\":1,\"server_id\":223344,\"event\":2}</code> and <code>sqlserver</code>'s last flushed offset is <code>{\"event_serial_no\":1,\"commit_lsn\":\"00000100:00000c00:0003\",\"change_lsn\":\"00000100:00000c00:0002\"}</code>. </p> <p>We should save this values regularly, so in case we run into a problem, we know the offset location in the past that can be set to resume the replication operation.</p>"},{"location":"user-guide/set_offset/#pause-the-connector","title":"Pause the Connector","text":"<p>A connector must be in a <code>paused</code> state before a new offset value can be set.</p> <p>Use <code>synchdb_pause_engine()</code> SQL function to pause a runnng connector. This will halt the Debezium runner engine from replicating from the heterogeneous database. When paused, it is possible to alter the Debezium connector's offset value to replicate from a specific point in the past using <code>synchdb_set_offset()</code> SQL routine. It takes <code>conninfo_name</code> as its argument which can be found from the output of <code>synchdb_get_state()</code> view.</p> <p>For example: <pre><code>SELECT synchdb_pause_engine('mysqlconn');\n</code></pre></p>"},{"location":"user-guide/set_offset/#set-the-new-offset","title":"Set the new Offset","text":"<p>Use <code>synchdb_set_offset()</code> SQL function to change a connector worker's starting offset. This can only be done when the connector is put into <code>paused</code> state. The function takes 2 parameters, <code>conninfo_name</code> and <code>a valid offset string</code>, both of which can be found from the output of <code>synchdb_get_state()</code> view.</p> <p>For example: <pre><code>SELECT synchdb_set_offset('mysqlconn', '{\"ts_sec\":1725644339,\"file\":\"mysql-bin.000004\",\"pos\":138466,\"row\":1,\"server_id\":223344,\"event\":2}');\n</code></pre></p>"},{"location":"user-guide/set_offset/#resume-the-connector","title":"Resume the Connector","text":"<p>Use <code>synchdb_resume_engine()</code> SQL function to resume Debezium operation from a paused state. This function takes <code>connector name</code> as its only parameter, which can be found from the output of <code>synchdb_get_state()</code> view. The resumed Debezium runner engine will start the replication from the newly set offset value.</p> <p>For example: <pre><code>SELECT synchdb_resume_engine('mysqlconn');\n</code></pre></p>"},{"location":"user-guide/utility_functions/","title":"Utility Function List","text":""},{"location":"user-guide/utility_functions/#synchdb_add_conninfo","title":"synchdb_add_conninfo","text":"<p>Used to create a new connector information: * <code>name</code> - a unique identifier that represents this connection info * <code>hostname</code> - the IP address of heterogeneous database. * <code>port</code> - the port number to connect to. * <code>username</code> - user name to use to connect. * <code>password</code> - password to authenticate the username. * <code>source database</code> (optional) - the database name to replicate changes from. For example, the database that exists in MySQL. If empty, all databases from MySQL are replicated. * <code>destination</code> database - the database to apply changes to - For example, the database that exists in PostgreSQL. Must be a valid database that exists. * <code>table</code> (optional) - expressed in the form of <code>[database].[table]</code> that must exists in MySQL so the engine will only replicate the specified tables. If empty, all tables are replicated. * <code>connector</code> - the connector to use (MySQL, Oracle, or SQLServer). Currently only MySQL and SQLServer are supported. * <code>rule file</code> - a JSON-formatted rule file placed under $PGDATA that this connector shall apply to its default data type translation rules. See below for more detail.</p> <p>Example:</p> <pre><code>SELECT synchdb_add_conninfo('mysqlconn','127.0.0.1',3306,'mysqluser', 'mysqlpwd', 'inventory', 'postgres', '', 'mysql', 'myrule.json');\n</code></pre>"},{"location":"user-guide/utility_functions/#synchdb_start_engine_bgw","title":"synchdb_start_engine_bgw","text":"<p>Used to start a connector: * <code>name</code> - the name of connector to start</p> <pre><code>SELECT synchdb_start_engine_bgw('mysqlconn');\n</code></pre>"},{"location":"user-guide/utility_functions/#synchdb_pause_engine","title":"synchdb_pause_engine","text":"<p>Used to pause a running connector: * <code>name</code> - the name of connector to pause</p> <pre><code>SELECT synchdb_pause_engine_bgw('mysqlconn');\n</code></pre>"},{"location":"user-guide/utility_functions/#synchdb_resume_engine","title":"synchdb_resume_engine","text":"<p>Used to resume a paused connector: * <code>name</code> - the name of connector to resume</p> <pre><code>SELECT synchdb_resume_engine('mysqlconn');\n</code></pre>"},{"location":"user-guide/utility_functions/#synchdb_stop_engine_bgw","title":"synchdb_stop_engine_bgw","text":"<p>Used to stop a paused or running connector: * <code>name</code> - the name of connector to stop</p> <pre><code>SELECT synchdb_stop_engine('mysqlconn');\n</code></pre>"},{"location":"user-guide/utility_functions/#synchdb_state_view","title":"synchdb_state_view","text":"<p>Used to examine all the running connectors and their states: <pre><code>SELECT * FROM synchdb_state_view();\n</code></pre></p> <ul> <li>id: unique identifier of a connector slot</li> <li>connector: the type of connector (mysql, oracle, sqlserver...etc)</li> <li>conninfo_name: the associated connection info name created by <code>synchdb_add_conninfo()</code></li> <li>pid: the PID of the connector worker process</li> <li>state: the state of the connector. Possible states are:<ul> <li>stopped</li> <li>initializing</li> <li>paused</li> <li>syncing</li> <li>parsing</li> <li>converting</li> <li>executing</li> <li>updating offset</li> <li>unknown</li> </ul> </li> <li>err: the last error message encountered by the worker which would have caused it to exit. This error could originated from PostgreSQL while processing a change, or originated from Debezium running engine while accessing data from heterogeneous database.</li> <li>last_dbz_offset: the last Debezium offset captured by synchdb. Note that this may not reflect the current and real-time offset value of the connector engine. Rather, this is shown as a checkpoint that we could restart from this offeet point if needed.</li> </ul>"},{"location":"user-guide/utility_functions/#synchdb_set_offset","title":"synchdb_set_offset","text":"<p>Used to set a custom starting offset value: * <code>name</code> - the name of connector to set a new offset * <code>offset</code> - the offset value to set</p> <pre><code>SELECT synchdb_set_offset('mysqlconn', '{\"ts_sec\":1725644339,\"file\":\"mysql-bin.000004\",\"pos\":138466,\"row\":1,\"server_id\":223344,\"event\":2}');\n</code></pre>"},{"location":"es/","title":"Bienvenido a SynchDB","text":"<p>SynchDB es una extensi\u00f3n de PostgreSQL para sincronizar datos de diferentes fuentes de bases de datos.</p>"},{"location":"es/#introduccion","title":"Introducci\u00f3n","text":"<p>SynchDB es una extensi\u00f3n de PostgreSQL dise\u00f1ada para replicar datos de una o m\u00e1s bases de datos heterog\u00e9neas (como MySQL, MS SQLServer, Oracle, etc.) directamente a PostgreSQL de una manera r\u00e1pida y confiable. PostgreSQL sirve como destino de m\u00faltiples fuentes de bases de datos heterog\u00e9neas. No se requiere ning\u00fan middleware ni software de terceros para orquestar la sincronizaci\u00f3n de datos entre bases de datos heterog\u00e9neas y PostgreSQL. La extensi\u00f3n SynchDB en s\u00ed es capaz de manejar todas las necesidades de sincronizaci\u00f3n de datos.</p> <p>Proporciona dos modos de trabajo clave que se pueden invocar utilizando las funciones SQL integradas: * Modo de sincronizaci\u00f3n (para la sincronizaci\u00f3n de datos inicial) * Modo de seguimiento (para replicar cambios incrementales despu\u00e9s de la sincronizaci\u00f3n inicial)</p> <p>El modo de sincronizaci\u00f3n copia tablas de bases de datos heterog\u00e9neas a PostgreSQL, incluido su esquema, \u00edndices, activadores, otras propiedades de tabla, as\u00ed como los datos actuales que contiene. El modo de seguimiento se suscribe a las tablas de una base de datos heterog\u00e9nea para obtener cambios incrementales y aplicarlos a las mismas tablas en PostgreSQL, de forma similar a la replicaci\u00f3n l\u00f3gica de PostgreSQL</p>"},{"location":"es/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Sincronizaci\u00f3n de datos eficiente</li> <li>Compatibilidad con m\u00faltiples fuentes de bases de datos</li> <li>Integraci\u00f3n sencilla con bases de datos PostgreSQL existentes</li> </ul>"},{"location":"es/#primeros-pasos","title":"Primeros pasos","text":"<p>Consulte nuestra Gu\u00eda de instalaci\u00f3n para comenzar a utilizar SynchDB.</p>"},{"location":"es/architecture/architecture/","title":"Arquitectura de SynchDB","text":""},{"location":"es/user-guide/installation/#requirement","title":"Requirement","text":"<p>The following software is required to build and run SynchDB. The versions listed are the versions tested during development. Older versions may still work. * Java Development Kit 22. Download here * Apache Maven 3.9.8. Download here * PostgreSQL 16.3 Source. Git clone here. Refer to this wiki for PostgreSQL build requirements * Docker compose 2.28.1 (for testing). Refer to here * Unix based operating system like Ubuntu 22.04 or MacOS</p>"},{"location":"es/user-guide/installation/#build-procedure","title":"Build Procedure","text":""},{"location":"es/user-guide/installation/#maven","title":"Maven","text":"<p>If you are working on Ubuntu 22.04.4 LTS, install the Maven as below: <pre><code>sudo apt install maven\n</code></pre></p> <p>if you are using MacOS, you can use the brew command to install maven (refer (here)[https://brew.sh/] for how to install Homebrew) without any other settings: <pre><code>brew install maven\n</code></pre></p>"},{"location":"es/user-guide/installation/#build-and-install-postgresql","title":"Build and Install PostgreSQL","text":"<p>This can be done by following the standard build and install procedure as described here. Generally, it consists of:</p> <pre><code>cd /home/$USER/postgres\n./configure\nmake\nsudo make install\n</code></pre> <p>You should build and install the default extensions as well: <pre><code>cd /home/$USER/postgres/contrib\nmake\nsudo make install\n</code></pre></p>"},{"location":"es/user-guide/installation/#build-debezium-runner-engine","title":"Build Debezium Runner Engine","text":"<p>With Java and Maven setup, we are ready to build Debezium Runner Engine. This installs the Debezium Runner Engine jar file to your PostgreSQL's lib folder.</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake build_dbz\nsudo make install_dbz\n</code></pre>"},{"location":"es/user-guide/installation/#build-synchdb-postgresql-extension","title":"Build SynchDB PostgreSQL Extension","text":"<p>With the Java <code>lib</code> and <code>include</code> installed in your system, SynchDB can be built by:</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake\nsudo make install\n</code></pre>"},{"location":"es/user-guide/installation/#configure-your-linker-ubuntu","title":"Configure your Linker (Ubuntu)","text":"<p>Lastly, we also need to tell your system's linker where the newly added Java library is located in your system.</p> <p><pre><code># Dynamically set JDK paths\nJAVA_PATH=$(which java)\nJDK_HOME_PATH=$(readlink -f ${JAVA_PATH} | sed 's:/bin/java::')\nJDK_LIB_PATH=${JDK_HOME_PATH}/lib\n\necho $JDK_LIB_PATH\necho $JDK_LIB_PATH/server\n\nsudo echo \"$JDK_LIB_PATH\" \uff5c sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\nsudo echo \"$JDK_LIB_PATH/server\" | sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\n</code></pre> Note, for mac with M1/M2 chips, you need to the two lines into /etc/ld.so.conf.d/aarch64-linux-gnu.conf</p> <p>Run ldconfig to reload: <pre><code>sudo ldconfig\n</code></pre></p> <p>Ensure synchdo.so extension can link to libjvm Java library on your system: <pre><code>ldd synchdb.so\n        linux-vdso.so.1 (0x00007ffeae35a000)\n        libjvm.so =&gt; /usr/lib/jdk-22.0.1/lib/server/libjvm.so (0x00007fc1276c1000)\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc127498000)\n        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc127493000)\n        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc12748e000)\n        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc127489000)\n        libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc1273a0000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fc128b81000)\n</code></pre></p>"},{"location":"zh/","title":"\u4e3b\u9875","text":""},{"location":"zh/#_2","title":"\u7b80\u4ecb","text":"<p>SynchDB \u662f\u4e00\u4e2a PostgreSQL \u6269\u5c55\uff0c\u65e8\u5728\u4ee5\u5feb\u901f\u53ef\u9760\u7684\u65b9\u5f0f\u5c06\u6570\u636e\u4ece\u4e00\u4e2a\u6216\u591a\u4e2a\u5f02\u6784\u6570\u636e\u5e93\uff08\u5982 MySQL\u3001MS SQLServer\u3001Oracle \u7b49\uff09\u76f4\u63a5\u590d\u5236\u5230 PostgreSQL\u3002PostgreSQL \u662f\u591a\u4e2a\u5f02\u6784\u6570\u636e\u5e93\u6e90\u7684\u76ee\u6807\u3002\u65e0\u9700\u4e2d\u95f4\u4ef6\u6216\u7b2c\u4e09\u65b9\u8f6f\u4ef6\u6765\u534f\u8c03\u5f02\u6784\u6570\u636e\u5e93\u548c PostgreSQL \u4e4b\u95f4\u7684\u6570\u636e\u540c\u6b65\u3002SynchDB \u6269\u5c55\u672c\u8eab\u80fd\u591f\u5904\u7406\u6240\u6709\u6570\u636e\u540c\u6b65\u9700\u6c42\u3002</p> <p>\u5b83\u63d0\u4f9b\u4e86\u4e24\u79cd\u53ef\u4ee5\u4f7f\u7528\u5185\u7f6e SQL \u51fd\u6570\u8c03\u7528\u7684\u5173\u952e\u5de5\u4f5c\u6a21\u5f0f\uff1a * \u540c\u6b65\u6a21\u5f0f\uff08\u7528\u4e8e\u521d\u59cb\u6570\u636e\u540c\u6b65\uff09 * \u8ddf\u968f\u6a21\u5f0f\uff08\u7528\u4e8e\u590d\u5236\u521d\u59cb\u540c\u6b65\u540e\u7684\u589e\u91cf\u66f4\u6539\uff09</p> <p>\u540c\u6b65\u6a21\u5f0f\u5c06\u8868\u4ece\u5f02\u6784\u6570\u636e\u5e93\u590d\u5236\u5230 PostgreSQL\uff0c\u5305\u62ec\u5176\u67b6\u6784\u3001\u7d22\u5f15\u3001\u89e6\u53d1\u5668\u3001\u5176\u4ed6\u8868\u5c5e\u6027\u4ee5\u53ca\u5b83\u6240\u4fdd\u5b58\u7684\u5f53\u524d\u6570\u636e\u3002 \u5173\u6ce8\u6a21\u5f0f\u8ba2\u9605\u5f02\u6784\u6570\u636e\u5e93\u4e2d\u7684\u8868\u4ee5\u83b7\u53d6\u589e\u91cf\u66f4\u6539\u5e76\u5c06\u5176\u5e94\u7528\u4e8e PostgreSQL \u4e2d\u7684\u76f8\u540c\u8868\uff0c\u7c7b\u4f3c\u4e8e PostgreSQL \u903b\u8f91\u590d\u5236</p>"},{"location":"zh/#_3","title":"\u529f\u80fd","text":"<ul> <li>\u9ad8\u6548\u7684\u6570\u636e\u540c\u6b65</li> <li>\u652f\u6301\u591a\u4e2a\u6570\u636e\u5e93\u6e90</li> <li>\u8f7b\u677e\u4e0e\u73b0\u6709 PostgreSQL \u6570\u636e\u5e93\u96c6\u6210</li> </ul>"},{"location":"zh/#_4","title":"\u5165\u95e8","text":"<p>\u67e5\u770b\u6211\u4eec\u7684 \u5b89\u88c5\u6307\u5357 \u4ee5\u5f00\u59cb\u4f7f\u7528 SynchDB\u3002</p>"},{"location":"zh/changelog/","title":"\u53d8\u66f4\u65e5\u5fd7","text":""},{"location":"zh/architecture/architecture/","title":"SynchDB \u67b6\u6784","text":""},{"location":"zh/user-guide/","title":"\u7528\u6237\u6307\u5357","text":""},{"location":"zh/user-guide/configuration/","title":"\u914d\u7f6e\u53c2\u6570","text":""},{"location":"zh/user-guide/connector_auto_launcher/","title":"\u8fde\u63a5\u5668\u81ea\u52a8\u542f\u52a8","text":""},{"location":"zh/user-guide/custom_datatype_mapping/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u7c7b\u578b\u6620\u5c04","text":""},{"location":"zh/user-guide/default_datatype_mapping/","title":"\u9ed8\u8ba4\u6570\u636e\u7c7b\u578b\u6620\u5c04","text":""},{"location":"zh/user-guide/installation/","title":"\u5b89\u88c5\u6307\u5357","text":""},{"location":"zh/user-guide/installation/#requirement","title":"Requirement","text":"<p>The following software is required to build and run SynchDB. The versions listed are the versions tested during development. Older versions may still work. * Java Development Kit 22. Download here * Apache Maven 3.9.8. Download here * PostgreSQL 16.3 Source. Git clone here. Refer to this wiki for PostgreSQL build requirements * Docker compose 2.28.1 (for testing). Refer to here * Unix based operating system like Ubuntu 22.04 or MacOS</p>"},{"location":"zh/user-guide/installation/#build-procedure","title":"Build Procedure","text":""},{"location":"zh/user-guide/installation/#maven","title":"Maven","text":"<p>If you are working on Ubuntu 22.04.4 LTS, install the Maven as below: <pre><code>sudo apt install maven\n</code></pre></p> <p>if you are using MacOS, you can use the brew command to install maven (refer (here)[https://brew.sh/] for how to install Homebrew) without any other settings: <pre><code>brew install maven\n</code></pre></p>"},{"location":"zh/user-guide/installation/#build-and-install-postgresql","title":"Build and Install PostgreSQL","text":"<p>This can be done by following the standard build and install procedure as described here. Generally, it consists of:</p> <pre><code>cd /home/$USER/postgres\n./configure\nmake\nsudo make install\n</code></pre> <p>You should build and install the default extensions as well: <pre><code>cd /home/$USER/postgres/contrib\nmake\nsudo make install\n</code></pre></p>"},{"location":"zh/user-guide/installation/#build-debezium-runner-engine","title":"Build Debezium Runner Engine","text":"<p>With Java and Maven setup, we are ready to build Debezium Runner Engine. This installs the Debezium Runner Engine jar file to your PostgreSQL's lib folder.</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake build_dbz\nsudo make install_dbz\n</code></pre>"},{"location":"zh/user-guide/installation/#build-synchdb-postgresql-extension","title":"Build SynchDB PostgreSQL Extension","text":"<p>With the Java <code>lib</code> and <code>include</code> installed in your system, SynchDB can be built by:</p> <pre><code>cd /home/$USER/postgres/contrib/synchdb\nmake\nsudo make install\n</code></pre>"},{"location":"zh/user-guide/installation/#configure-your-linker-ubuntu","title":"Configure your Linker (Ubuntu)","text":"<p>Lastly, we also need to tell your system's linker where the newly added Java library is located in your system.</p> <p><pre><code># Dynamically set JDK paths\nJAVA_PATH=$(which java)\nJDK_HOME_PATH=$(readlink -f ${JAVA_PATH} | sed 's:/bin/java::')\nJDK_LIB_PATH=${JDK_HOME_PATH}/lib\n\necho $JDK_LIB_PATH\necho $JDK_LIB_PATH/server\n\nsudo echo \"$JDK_LIB_PATH\" \uff5c sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\nsudo echo \"$JDK_LIB_PATH/server\" | sudo tee -a /etc/ld.so.conf.d/x86_64-linux-gnu.conf\n</code></pre> Note, for mac with M1/M2 chips, you need to the two lines into /etc/ld.so.conf.d/aarch64-linux-gnu.conf</p> <p>Run ldconfig to reload: <pre><code>sudo ldconfig\n</code></pre></p> <p>Ensure synchdo.so extension can link to libjvm Java library on your system: <pre><code>ldd synchdb.so\n        linux-vdso.so.1 (0x00007ffeae35a000)\n        libjvm.so =&gt; /usr/lib/jdk-22.0.1/lib/server/libjvm.so (0x00007fc1276c1000)\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc127498000)\n        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc127493000)\n        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc12748e000)\n        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc127489000)\n        libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc1273a0000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fc128b81000)\n</code></pre></p>"},{"location":"zh/user-guide/prepare_tests_env/","title":"\u8bbe\u7f6e\u6d4b\u8bd5\u73af\u5883","text":""},{"location":"zh/user-guide/quick_start/","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"zh/user-guide/set_offset/","title":"\u8bbe\u7f6e\u504f\u79fb\u91cf","text":""},{"location":"zh/user-guide/usage/","title":"Usage","text":""},{"location":"zh/user-guide/utility_functions/","title":"\u5b9e\u7528\u529f\u80fd","text":""}]}